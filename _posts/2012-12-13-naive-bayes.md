---
layout: post
title: Наивный байесовский классификатор
tags: [naive, bayes, classifier, mining]
---

[Наивный байесовский классификатор](http://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9_%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80) - классификатор, на самом деле очень простой, за кучей формул скрывается очень простая идея.

Заметка написана по мотивам этой статьи http://bazhenov.me/blog/2012/06/11/naive-bayes.html

Предположим у нас есть следующая база знаний, о принадлежности сообщений к спаму:

**SPAM**

 * предоставляю услуги бухгалтера
 * спешите купить виагру

**NOT SPAM**

 * надо купить молоко

**Формула**

    log(Dc/D) + foreach(word) { log( (Wc+1)/(V+Lc) ) }

Где:

 * `Dc` - количество **документов** в обучающей выборке принадлежащих классу `c` (2 SPAM документа и 1 - NOT SPAM)
 * `D` - общее количество **документов** в обучающей выборке (всего 3 документа)
 * `V` - общее количество **слов** во всех документах обучающей выборки (всего слов во всех документах - 8)
 * `Lc` - суммарное количество **слов** в документах класса `c` (искомого типа) в обучающей выборке (в SPAM документах - 6 слов, в NOT SPAM - 3)
 * `Wc` - сколько раз **слово** встречалось в документах класса `c` (искомого типа) в обучающей выборке

**Таблица слов**

    WORD            SPAM    NOT SPAM
    --------------------------------
    предоставляю    1       0
    услуги          1       0
    бухгалтера      1       0
    спешите         1       0
    купить          1       1
    виагру          1       0
    надо            0       1
    молоко          0       1

Другими словами мы просто разбили все обучаемые фразы на слова и записали их принадлежность к тому или иному классу (SPAM, NOT SPAM)

Предположим мы хотим проверить фразу: "надо купить сигареты"

Нам нужно посчитать вероятность принадлежности текста как к спаму так и в к не спаму.

SPAM
----

    log(Dc/D) + foreach(word) { log( (Wc+1)/(V+Lc) ) }

`log(Dc/D)` - эдакая константа, в обучающей выборке 2 spam документа из 3, соотв. это дело будет записано как `log(2/3)`

`V+Lc` - будет одинаковым для всех слов, `V` - 8 - именно столько всего слов в обучающей выборке, `Lc` - 6 - столько слов в обучающей выборке было в спамовых сообщениях.

Далее для каждого слова, считаем log.

"надо" тут у нас `Wc` равно 0, так как это слово не встречалось в обучающей выборке среди спама
"купить" тут у нас `Wc` равно 1, так как это слово один раз встречалось в обучающей выборке среди спама
"сигареты" тут у нас `Wc` равно 0, так как это слово не встречалось в обучающей выборке среди спама

Результат:

    log(2/3)
    + log( (0 + 1)/(8+6) ) //надо
    + log( (1 + 1)/(8+6) ) //купить
    + log( (0 + 1)/(8+6) ) //сигареты
    = -7.629

NOT SPAM
--------

Теперь все то же самое для не спама

    log(Dc/D) + foreach(word) { log( (Wc+1)/(V+Lc) ) }

`log(Dc/D)` - эдакая константа, в обучающей выборке 1 not spam документа из 3, соотв. это дело будет записано как `log(1/3)`

`V+Lc` - будет одинаковым для всех слов, `V` - 8 - именно столько всего слов в обучающей выборке, `Lc` - 3 - столько слов в обучающей выборке было в NOT SPAM сообщениях.

Далее для каждого слова, считаем log.

"надо" тут у нас `Wc` равно 1, так как это слово один раз встречалось в обучающей выборке среди not spam
"купить" тут у нас `Wc` равно 1, так как это слово один раз встречалось в обучающей выборке среди not spam
"сигареты" тут у нас `Wc` равно 0, так как это слово не встречалось в обучающей выборке среди not spam

Результат:

    log(1/3)
    + log( (1 + 1)/(8+3) ) //надо
    + log( (1 + 1)/(8+3) ) //купить
    + log( (0 + 1)/(8+3) ) //сигареты
    = -6.906

Из чего делаем вывод что фраза "надо купить сигареты" с большей вероятностью относиться к нормальным сообщениям нежели к spam'у.

Вот так можно на листике бумаги расписать всю работу алгоритма.

Если добавить в базу знаний к спам сообщенияе что нибуть вроде "надо купить виагру" - то результат поменяется в обратную сторону.

Все это дело можно очень просто сделать на любом языке, вот пример (не претендующий на крутость, просто чтобы показать и запомнить как оно работает) http://mac-blog.org.ua/examples/bayes.html

Все это дело расковыривалось в процессе ресерча возможности заюзать встроенный в SQL Server классификатор документов (там все это работает из коробки), но к сожалению оказалось что тамошний term extract и term lookup умеют работать только с английским языком, что свело на нет все надежды.
